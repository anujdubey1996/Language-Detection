{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras_wc_embd import get_dicts_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "CUDA_VISIBLE_DEVICES = 0,1\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = open('F:/Datasets/Language Identification/x_train.txt', 'r', encoding='utf-8').readlines()\n",
    "sentences2 = open('F:/Datasets/Language Identification/x_test.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = pd.DataFrame(sentences1, columns=['Sentence'])\n",
    "sentences2 = pd.DataFrame(sentences2, columns=['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.concat([sentences1, sentences2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = open('F:/Datasets/Language Identification/y_train.txt', 'r', encoding='utf-8').readlines()\n",
    "labels2 = open('F:/Datasets/Language Identification/y_test.txt', 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = pd.DataFrame(labels1, columns=['Language'])\n",
    "labels2 = pd.DataFrame(labels2, columns=['Language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([labels1, labels2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataset = pd.merge(sentences.reset_index(drop=True), labels.reset_index(drop=True), left_index=True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_languages = ['eng\\n', 'spa\\n', 'deu\\n', 'ita\\n', 'fra\\n', 'rus\\n', 'ara\\n']#list(np.unique(language_dataset['Language']))\n",
    "language_dataset = language_dataset[language_dataset['Language'].isin(unique_languages)]\n",
    "language_dataset['Sentence Tokenized'] = language_dataset['Sentence'].apply(lambda x : nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = language_dataset['Sentence Tokenized'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "model = Word2Vec(sentences, size = 300, window = 5, min_count= 1, negative=5, iter = 100)\n",
    "time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.25551962852478"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2-time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('MultiLingualModel.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_languages = list(np.unique(language_dataset['Language']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataset = language_dataset.drop(['Sentence Tokenized'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataset_part1 = language_dataset#[language_dataset['Language'].isin(unique_languages[0:5])]\n",
    "#language_dataset_part2 = language_dataset[language_dataset['Language'].isin(unique_languages[50:100])]\n",
    "#language_dataset_part3 = language_dataset[language_dataset['Language'].isin(unique_languages[100:150])]\n",
    "#language_dataset_part3 = language_dataset[language_dataset['Language'].isin(unique_languages[150:200])]\n",
    "#language_dataset_part3 = language_dataset[language_dataset['Language'].isin(unique_languages[200:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2token(word):\n",
    "    try:\n",
    "        return model.wv.vocab[word].index\n",
    "    # If word is not in index return 0. I realize this means that this\n",
    "    # is the same as the word of index 0 (i.e. most frequent word), but 0s\n",
    "    # will be padded later anyway by the embedding layer (which also\n",
    "    # seems dirty but I couldn't find a better solution right now)\n",
    "    except KeyError:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 200)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "#Create an iterator that formats data from the dataset proper for\n",
    "# LSTM training\n",
    "\n",
    "# Sequences will be padded or truncated to this length\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# Samples of categories with less than this number of samples will be ignored\n",
    "DROP_THRESHOLD = 10000\n",
    "\n",
    "class SequenceIterator:\n",
    "    def __init__(self, dataset, drop_threshold, seq_length):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '–')\n",
    "        self.categories, self.ccount = np.unique(dataset.Language, return_counts=True)\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for sent, lang in zip(self.dataset.Sentence, self.dataset.Language):\n",
    "            # Make all characters lower-case\n",
    "            sent = sent.lower()\n",
    "            \n",
    "            # Clean string of all punctuation\n",
    "            sent = sent.translate(self.translator)\n",
    "\n",
    "            words = np.array([word2token(w) for w in sent.split(' ')[:self.seq_length] if w != ''])\n",
    "                                \n",
    "            yield (words, lang)\n",
    "\n",
    "sequences = SequenceIterator(language_dataset_part1, DROP_THRESHOLD, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Used for generating the labels in the set\n",
    "cat_dict = {k: v for k, v in zip(sequences.categories, range(len(sequences.categories)))}\n",
    "\n",
    "set_x = []\n",
    "set_y = []\n",
    "for w, c in sequences:\n",
    "    set_x.append(w)\n",
    "    set_y.append(cat_dict[c])\n",
    "    \n",
    "# Padding sequences with 0.\n",
    "set_x = pad_sequences(set_x, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)\n",
    "set_y = np.array(set_y)\n",
    "\n",
    "print(set_x.shape)\n",
    "print(set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shapes - X: (6650, 200) - Y: (6650,)\n",
      "Val Shapes - X: (350, 200) - Y: (350,)\n"
     ]
    }
   ],
   "source": [
    "VALID_PER = 0.05 # Percentage of the whole set that will be separated for validation\n",
    "\n",
    "total_samples = set_x.shape[0]\n",
    "n_val = int(VALID_PER * total_samples)\n",
    "n_train = total_samples - n_val\n",
    "\n",
    "random_i = random.sample(range(total_samples), total_samples)\n",
    "train_x = set_x[random_i[:n_train]]\n",
    "train_y = set_y[random_i[:n_train]]\n",
    "val_x = set_x[random_i[n_train:n_train+n_val]]\n",
    "val_y = set_y[random_i[n_train:n_train+n_val]]\n",
    "\n",
    "print(\"Train Shapes - X: {} - Y: {}\".format(train_x.shape, train_y.shape))\n",
    "print(\"Val Shapes - X: {} - Y: {}\".format(val_x.shape, val_y.shape))\n",
    "\n",
    "categories, ccount = np.unique(train_y, return_counts=True)\n",
    "n_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6650 samples, validate on 350 samples\n",
      "Epoch 1/5\n",
      "6650/6650 [==============================] - 31s 5ms/sample - loss: 0.1083 - accuracy: 0.9795 - val_loss: 0.0714 - val_accuracy: 0.9829\n",
      "Epoch 2/5\n",
      "6650/6650 [==============================] - 25s 4ms/sample - loss: 0.0229 - accuracy: 0.9941 - val_loss: 0.0734 - val_accuracy: 0.9829\n",
      "Epoch 3/5\n",
      "6650/6650 [==============================] - 25s 4ms/sample - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.0806 - val_accuracy: 0.9829\n",
      "Epoch 4/5\n",
      "6650/6650 [==============================] - 25s 4ms/sample - loss: 0.0067 - accuracy: 0.9986 - val_loss: 0.0841 - val_accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "6650/6650 [==============================] - 25s 4ms/sample - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0883 - val_accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "w2v_weights = model.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# Keras Embedding layer with Word2Vec weights initialization\n",
    "lstm_model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=300,\n",
    "                    weights=[w2v_weights],\n",
    "                    input_length=200,\n",
    "                    mask_zero=True,\n",
    "                    trainable=False))\n",
    "\n",
    "lstm_model.add(Bidirectional(LSTM(100)))\n",
    "lstm_model.add(Dense(n_categories, activation='softmax'))\n",
    "lstm_model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = lstm_model.fit(train_x, train_y, epochs=5, batch_size=64,\n",
    "                    validation_data=(val_x, val_y), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_format_output(X):\n",
    "    string1 = X\n",
    "    string1 = pd.DataFrame(string1, columns =['Sentence'])\n",
    "    string1['Language'] = \"X\"\n",
    "\n",
    "    sequence1 = SequenceIterator(string1, DROP_THRESHOLD, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Used for generating the labels in the set\n",
    "    cat_dict = {k: v for k, v in zip(sequence1.categories, range(len(sequence1.categories)))}\n",
    "\n",
    "    set_x = []\n",
    "    set_y = []\n",
    "    for w, c in sequence1:\n",
    "        set_x.append(w)\n",
    "        set_y.append(cat_dict[c])\n",
    "\n",
    "    # Padding sequences with 0.\n",
    "    set_x = pad_sequences(set_x, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)\n",
    "    return set_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(X, lstm_model):\n",
    "    X = [X]\n",
    "    X = make_lstm_format_output(X)\n",
    "    output = lstm_model.model.predict(X)\n",
    "    ind = np.argmax(output)\n",
    "    lang = list(cat_dict.keys())\n",
    "    lang = ['Arabic', 'Deutsche', 'English', 'French', 'Italian', 'Russian', 'Spanish']\n",
    "    print(lang[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutsche\n",
      "English\n",
      "Spanish\n",
      "French\n",
      "Russian\n",
      "Arabic\n"
     ]
    }
   ],
   "source": [
    "predict_language('Darf ich mal vorbei?', history)\n",
    "predict_language('Hey Pal. What is up?', history)\n",
    "predict_language('Buenos días, Estela!', history)\n",
    "predict_language('Parlez-vous anglais?', history)\n",
    "predict_language('Добрый день', history)\n",
    "predict_language('السلام عليكم', history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
